{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import urllib.request\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, LeakyReLU, PReLU\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.backend import set_session\n",
    "#from keras.backend.tensorflow_backend import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coins = ['BTC', 'LTC', 'ETH', 'XRP']\n",
    "coins = ['XRP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio of train to test split (originally at 0.8)\n",
    "train_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from time import mktime\n",
    "\n",
    "delta_days = 90\n",
    "\n",
    "t = datetime.now()\n",
    "end_tm = np.int(mktime(t.timetuple()))\n",
    "tpast = t - timedelta(days=delta_days)\n",
    "start_tm = np.int(mktime(tpast.timetuple()))\n",
    "#start_tm = mktime((t - timedelta(days=delta_days)).tuple())\n",
    "period_step = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25920, 8)\n",
      "Returned length of 1 coins: (25920, 9)\n"
     ]
    }
   ],
   "source": [
    "df_list=[]\n",
    "\n",
    "container = []\n",
    "\n",
    "for coin in coins:\n",
    "    url = 'https://poloniex.com/public?command=returnChartData&currencyPair=USDT_'+coin+'&start={0}&end={1}&period={2}'.format(start_tm, end_tm, period_step)\n",
    "    openUrl = urllib.request.urlopen(url)\n",
    "    r = openUrl.read()\n",
    "    openUrl.close()\n",
    "    d = json.loads(r.decode())\n",
    "    dfx = pd.DataFrame(d)\n",
    "    dfx['coin'] = coin\n",
    "    container.append(dfx)\n",
    "    print(pd.DataFrame(d).shape)\n",
    "    \n",
    "container = pd.concat(container, axis = 0)\n",
    "print('Returned length of {0} coins: {1}'.format(len(coins), container.shape))\n",
    "\n",
    "\n",
    "# format\n",
    "original_columns=[u'close', u'date', u'high', u'low', u'open']\n",
    "new_columns = ['Close','Timestamp','High','Low','Open']\n",
    "container = container.loc[:,original_columns]\n",
    "container.columns = new_columns\n",
    "container.to_csv('data/bitcoin2015to2017.csv',index=None)\n",
    "\n",
    "# export out\n",
    "h5path = 'data/4coins_17apr2021.h5'\n",
    "container.to_hdf(h5path, key='df', mode='w')\n",
    "#container.to_csv('data/4coins_17apr2021.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "class PastSampler:\n",
    "    '''\n",
    "    Forms training samples for predicting future values from past value\n",
    "    '''\n",
    "     \n",
    "    def __init__(self, N, K, sliding_window = True):\n",
    "        '''\n",
    "        Predict K future sample using N previous samples\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.sliding_window = sliding_window\n",
    " \n",
    "    def transform(self, A):\n",
    "        M = self.N + self.K     #Number of samples per row (sample + target)\n",
    "        #indexes\n",
    "        if self.sliding_window:\n",
    "            I = np.arange(M) + np.arange(A.shape[0] - M + 1).reshape(-1, 1)\n",
    "        else:\n",
    "            if A.shape[0]%M == 0:\n",
    "                I = np.arange(M)+np.arange(0,A.shape[0],M).reshape(-1,1)\n",
    "                \n",
    "            else:\n",
    "                I = np.arange(M)+np.arange(0,A.shape[0] -M,M).reshape(-1,1)\n",
    "            \n",
    "        B = A[I].reshape(-1, M * A.shape[1], A.shape[2])\n",
    "        ci = self.N * A.shape[1]    #Number of features per sample\n",
    "        return B[:, :ci], B[:, ci:] #Sample matrix, Target matrix\n",
    "\n",
    "#data file path\n",
    "dfp = 'data/bitcoin2015to2017.csv'\n",
    "\n",
    "#Columns of price data to use\n",
    "columns = ['Close']\n",
    "df = pd.read_csv(dfp)\n",
    "time_stamps = df['Timestamp']\n",
    "df = df.loc[:,columns]\n",
    "original_df = pd.read_csv(dfp).loc[:,columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='bitcoin2015to2017_close.h5'\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# normalization\n",
    "for c in columns:\n",
    "    df[c] = scaler.fit_transform(df[c].values.reshape(-1,1))\n",
    "    \n",
    "#Features are input sample dimensions(channels)\n",
    "A = np.array(df)[:,None,:]\n",
    "original_A = np.array(original_df)[:,None,:]\n",
    "time_stamps = np.array(time_stamps)[:,None,None]\n",
    "\n",
    "#Make samples of temporal sequences of pricing data (channel)\n",
    "NPS, NFS = 256, 16        #Number of past and future samples  -original at 16\n",
    "ps = PastSampler(NPS, NFS, sliding_window=True)\n",
    "B, Y = ps.transform(A)\n",
    "input_times, output_times = ps.transform(time_stamps)\n",
    "original_B, original_Y = ps.transform(original_A)\n",
    "\n",
    "import h5py\n",
    "with h5py.File(file_name, 'w') as f:\n",
    "    f.create_dataset(\"inputs\", data = B)\n",
    "    f.create_dataset('outputs', data = Y)\n",
    "    f.create_dataset(\"input_times\", data = input_times)\n",
    "    f.create_dataset('output_times', data = output_times)\n",
    "    f.create_dataset(\"original_datas\", data=np.array(original_df))\n",
    "    f.create_dataset('original_inputs',data=original_B)\n",
    "    f.create_dataset('original_outputs',data=original_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 25920 is out of bounds for axis 0 with size 25920",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-a1e34d99571c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtime_stamps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25920\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 25920 is out of bounds for axis 0 with size 25920"
     ]
    }
   ],
   "source": [
    "time_stamps[25920]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_times[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_times[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1618720500 - 1618691400)/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_times[94][255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1618696200 - 1618691400)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_times[94][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time_stamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make the program use only one GPU\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "with h5py.File(''.join(['bitcoin2015to2017_close.h5']), 'r') as hf:\n",
    "    datas = hf['inputs'].value\n",
    "    labels = hf['outputs'].value\n",
    "\n",
    "\n",
    "output_file_name='bitcoin2015to2017_close_CNN_2_relu'\n",
    "\n",
    "step_size = datas.shape[1]\n",
    "batch_size= 8\n",
    "nb_features = datas.shape[2]\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "#split training validation\n",
    "training_size = int(train_ratio* datas.shape[0])\n",
    "training_datas = datas[:training_size,:]\n",
    "training_labels = labels[:training_size,:]\n",
    "validation_datas = datas[training_size:,:]\n",
    "validation_labels = labels[training_size:,:]\n",
    "#build model\n",
    "\n",
    "# 2 layers\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=3, filters=8, kernel_size=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D( strides=4, filters=nb_features, kernel_size=16))\n",
    "\n",
    "'''\n",
    "# 3 Layers\n",
    "model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=3, filters=8, kernel_size=8))\n",
    "#model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(activation='relu', strides=2, filters=8, kernel_size=8))\n",
    "#model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D( strides=2, filters=nb_features, kernel_size=8))\n",
    "# 4 layers\n",
    "model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=2, filters=8, kernel_size=2))\n",
    "#model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(activation='relu', strides=2, filters=8, kernel_size=2))\n",
    "#model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(activation='relu', strides=2, filters=8, kernel_size=2))\n",
    "#model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D( strides=2, filters=nb_features, kernel_size=2))\n",
    "'''\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(training_datas, training_labels,verbose=1, batch_size=batch_size,validation_data=(validation_datas,validation_labels), epochs = epochs, callbacks=[CSVLogger(output_file_name+'.csv', append=True),ModelCheckpoint('weights/'+output_file_name+'-{epoch:02d}-{val_loss:.5f}.hdf5', monitor='val_loss', verbose=1,mode='min')])\n",
    "\n",
    "# Pick latest model weights\n",
    "import glob\n",
    "import os\n",
    "\n",
    "list_of_files = glob.glob('weights/*') # * means all if need specific format then *.csv\n",
    "latest_weights = max(list_of_files, key=os.path.getctime)\n",
    "print(latest_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "list_of_files = glob.glob('weights/*') # * means all if need specific format then *.csv\n",
    "latest_weights = max(list_of_files, key=os.path.getctime)\n",
    "print(latest_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.callbacks import CSVLogger\n",
    "import tensorflow as tf\n",
    "#from scipy.ndimage import imread\n",
    "#import scipy.imageio.imread\n",
    "#from scipy.misc.pilutil import imread\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, LeakyReLU\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "from keras import optimizers\n",
    "import h5py\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import pandas as pd\n",
    "# import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "with h5py.File(''.join(['bitcoin2015to2017_close.h5']), 'r') as hf:\n",
    "    datas = hf['inputs'].value\n",
    "    labels = hf['outputs'].value\n",
    "    input_times = hf['input_times'].value\n",
    "    output_times = hf['output_times'].value\n",
    "    original_inputs = hf['original_inputs'].value\n",
    "    original_outputs = hf['original_outputs'].value\n",
    "    original_datas = hf['original_datas'].value\n",
    "    \n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "#split training validation\n",
    "training_size = int(train_ratio* datas.shape[0])\n",
    "training_datas = datas[:training_size,:,:]\n",
    "training_labels = labels[:training_size,:,:]\n",
    "validation_datas = datas[training_size:,:,:]\n",
    "validation_labels = labels[training_size:,:,:]\n",
    "validation_original_outputs = original_outputs[training_size:,:,:]\n",
    "validation_original_inputs = original_inputs[training_size:,:,:]\n",
    "validation_input_times = input_times[training_size:,:,:]\n",
    "validation_output_times = output_times[training_size:,:,:]\n",
    "\n",
    "ground_true = np.append(validation_original_inputs,validation_original_outputs, axis=1)\n",
    "ground_true_times = np.append(validation_input_times,validation_output_times, axis=1)\n",
    "step_size = datas.shape[1]\n",
    "batch_size= 15\n",
    "nb_features = datas.shape[2]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 2 layers\n",
    "model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=3, filters=8, kernel_size=20))\n",
    "# model.add(LeakyReLU())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D( strides=4, filters=nb_features, kernel_size=16))\n",
    "model.load_weights('{0}'.format(latest_weights))\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(validation_datas)\n",
    "predicted_inverted = []\n",
    "\n",
    "for i in range(original_datas.shape[1]):\n",
    "    scaler.fit(original_datas[:,i].reshape(-1,1))\n",
    "    predicted_inverted.append(scaler.inverse_transform(predicted[:,:,i]))\n",
    "print (np.array(predicted_inverted).shape)\n",
    "#get only the close data\n",
    "ground_true = ground_true[:,:,0].reshape(-1)\n",
    "ground_true_times = ground_true_times.reshape(-1)\n",
    "ground_true_times = pd.to_datetime(ground_true_times, unit='s')\n",
    "# since we are appending in the first dimension\n",
    "predicted_inverted = np.array(predicted_inverted)[0,:,:].reshape(-1)\n",
    "print (np.array(predicted_inverted).shape)\n",
    "validation_output_times = pd.to_datetime(validation_output_times.reshape(-1), unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_local(df, col_from = 'Timestamp', col_to = 'times',local_timezone = 'Singapore'):\n",
    "    '''convert a datetime time unaware field to local\n",
    "    '''\n",
    "    from datetime import timezone, datetime\n",
    "\n",
    "    # 3rd party modules\n",
    "    import pandas as pd\n",
    "    import pytz\n",
    "    \n",
    "    df[col_to] = pd.to_datetime(df[col_from], unit='s')\n",
    "    df[col_to] = df[col_to].dt.tz_localize(timezone.utc)\n",
    "\n",
    "    my_timezone = pytz.timezone(local_timezone)\n",
    "    df[col_to] = df[col_to].dt.tz_convert(my_timezone)\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_true_df = pd.DataFrame()\n",
    "ground_true_df['times'] = ground_true_times\n",
    "ground_true_df['value'] = ground_true\n",
    "\n",
    "prediction_df = pd.DataFrame()\n",
    "prediction_df['times'] = validation_output_times\n",
    "prediction_df['value'] = predicted_inverted\n",
    "\n",
    "\n",
    "# bring to local time\n",
    "ground_true_df = change_to_local(ground_true_df, 'times','times', 'Singapore')\n",
    "prediction_df= change_to_local(prediction_df, 'times','times', 'Singapore')\n",
    "\n",
    "# build singular dataset\n",
    "all_df = ground_true_df.merge(prediction_df, on = 'times', how = 'outer')\n",
    "all_df.columns = ['times', 'actual', 'predicted']\n",
    "\n",
    "#prediction_df = prediction_df.loc[(prediction_df[\"times\"].dt.year == 2017 )&(prediction_df[\"times\"].dt.month > 7 ),: ]\n",
    "#ground_true_df = ground_true_df.loc[(ground_true_df[\"times\"].dt.year == 2017 )&(ground_true_df[\"times\"].dt.month > 7 ),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check original loaded data\n",
    "container = change_to_local(container)\n",
    "container.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate to date\n",
    "gt = ground_true_df.groupby(pd.to_datetime(ground_true_df.times).dt.date)[['value']].mean().reset_index()\n",
    "gt.columns = ['times', 'actual']\n",
    "pt = prediction_df.groupby(pd.to_datetime(prediction_df.times).dt.date)[['value']].mean().reset_index()\n",
    "pt.columns = ['times', 'predicted']\n",
    "allt = gt.merge(pt, on = 'times', how = 'outer')\n",
    "\n",
    "allt.set_index('times').plot(figsize=(20,5), grid=True, label = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(ground_true_df.times,ground_true_df.value, label = 'Actual')\n",
    "plt.plot(prediction_df.times,prediction_df.value,'ro--', label='Predicted')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
